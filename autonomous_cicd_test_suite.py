#!/usr/bin/env python3
"""
Autonomous CI/CD Test Suite for GenRF

Production-ready test suite designed for automated CI/CD pipelines.
Includes comprehensive validation, performance benchmarks, and quality gates.
"""

import sys
import os
import json
import time
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any
import tempfile
import hashlib

class CICDTestRunner:
    """Autonomous CI/CD test runner with comprehensive validation."""
    
    def __init__(self):
        self.start_time = datetime.now()
        self.test_results = {}
        self.performance_metrics = {}
        self.quality_gates = {}
        self.project_root = Path(__file__).parent
        
    def run_all_tests(self) -> Dict[str, Any]:
        """Execute complete test suite with quality gates."""
        print("üöÄ GenRF Autonomous CI/CD Test Suite")
        print("=" * 60)
        
        # Core functionality tests
        self._test_code_quality()
        self._test_import_structure()
        self._test_lightweight_functionality()
        self._test_performance_benchmarks()
        self._test_security_validation()
        self._test_documentation_completeness()
        
        # Execute quality gates
        self._execute_quality_gates()
        
        # Generate comprehensive report
        return self._generate_test_report()
    
    def _test_code_quality(self):
        """Test code quality and style compliance."""
        print("\nüìã Testing Code Quality...")
        
        try:
            # Check Python syntax
            syntax_errors = 0
            python_files = list(self.project_root.glob("**/*.py"))
            
            for py_file in python_files:
                try:
                    with open(py_file) as f:
                        compile(f.read(), py_file, 'exec')
                except SyntaxError:
                    syntax_errors += 1
            
            # Check file structure
            required_files = [
                "README.md", "requirements.txt", "pyproject.toml",
                "genrf/__init__.py", "genrf/core/__init__.py"
            ]
            
            missing_files = [f for f in required_files if not (self.project_root / f).exists()]
            
            # Code metrics
            total_lines = sum(1 for f in python_files for _ in open(f))
            
            self.test_results["code_quality"] = {
                "status": "PASS" if syntax_errors == 0 and len(missing_files) == 0 else "FAIL",
                "syntax_errors": syntax_errors,
                "missing_files": missing_files,
                "total_python_files": len(python_files),
                "total_lines_of_code": total_lines
            }
            
            print(f"  ‚úÖ Python files: {len(python_files)}")
            print(f"  ‚úÖ Lines of code: {total_lines}")
            print(f"  ‚úÖ Syntax errors: {syntax_errors}")
            print(f"  ‚úÖ Missing files: {len(missing_files)}")
            
        except Exception as e:
            self.test_results["code_quality"] = {"status": "FAIL", "error": str(e)}
            print(f"  ‚ùå Code quality test failed: {e}")
    
    def _test_import_structure(self):
        """Test package import structure and dependencies."""
        print("\nüì¶ Testing Import Structure...")
        
        try:
            import_tests = {
                "basic_imports": self._test_basic_imports(),
                "core_modules": self._test_core_modules(),
                "circular_dependencies": self._test_circular_dependencies()
            }
            
            all_passed = all(result["status"] == "PASS" for result in import_tests.values())
            
            self.test_results["import_structure"] = {
                "status": "PASS" if all_passed else "FAIL",
                "details": import_tests
            }
            
            for test_name, result in import_tests.items():
                status = "‚úÖ" if result["status"] == "PASS" else "‚ùå"
                print(f"  {status} {test_name.replace('_', ' ').title()}")
                
        except Exception as e:
            self.test_results["import_structure"] = {"status": "FAIL", "error": str(e)}
            print(f"  ‚ùå Import structure test failed: {e}")
    
    def _test_basic_imports(self) -> Dict[str, Any]:
        """Test basic Python imports without heavy dependencies."""
        try:
            import importlib.util
            
            # Test lightweight modules
            modules_to_test = [
                "genrf.core.design_spec",
                "genrf.core.exceptions", 
                "genrf.core.validation"
            ]
            
            importable_modules = 0
            for module_name in modules_to_test:
                try:
                    module_path = self.project_root / module_name.replace(".", "/") + ".py"
                    if module_path.exists():
                        spec = importlib.util.spec_from_file_location(module_name, module_path)
                        if spec:
                            importable_modules += 1
                except:
                    pass
            
            return {
                "status": "PASS" if importable_modules > 0 else "FAIL",
                "importable_modules": importable_modules,
                "total_tested": len(modules_to_test)
            }
            
        except Exception as e:
            return {"status": "FAIL", "error": str(e)}
    
    def _test_core_modules(self) -> Dict[str, Any]:
        """Test core module structure."""
        try:
            core_modules = list((self.project_root / "genrf" / "core").glob("*.py"))
            module_count = len([m for m in core_modules if m.name != "__init__.py"])
            
            return {
                "status": "PASS" if module_count >= 5 else "FAIL",
                "core_modules_found": module_count,
                "modules": [m.name for m in core_modules]
            }
            
        except Exception as e:
            return {"status": "FAIL", "error": str(e)}
    
    def _test_circular_dependencies(self) -> Dict[str, Any]:
        """Test for circular import dependencies."""
        try:
            # Simple heuristic: check if __init__.py files are not too complex
            init_files = list(self.project_root.glob("**/__init__.py"))
            complex_inits = []
            
            for init_file in init_files:
                with open(init_file) as f:
                    lines = f.readlines()
                    import_lines = [l for l in lines if l.strip().startswith(("from .", "import ."))]
                    if len(import_lines) > 20:  # Arbitrary threshold
                        complex_inits.append(init_file.name)
            
            return {
                "status": "PASS" if len(complex_inits) == 0 else "WARN",
                "complex_init_files": complex_inits
            }
            
        except Exception as e:
            return {"status": "FAIL", "error": str(e)}
    
    def _test_lightweight_functionality(self):
        """Test lightweight functionality without ML dependencies."""
        print("\nüß™ Testing Lightweight Functionality...")
        
        try:
            # Run the lightweight demo
            start_time = time.time()
            result = subprocess.run([
                sys.executable, "genrf_lightweight_demo.py"
            ], capture_output=True, text=True, cwd=self.project_root)
            execution_time = time.time() - start_time
            
            # Parse output for success indicators
            output = result.stdout
            success_indicators = [
                "ALL TESTS PASSED",
                "DEMONSTRATION COMPLETED SUCCESSFULLY",
                "5/5 tests passed (100.0%)"
            ]
            
            success_count = sum(1 for indicator in success_indicators if indicator in output)
            
            self.test_results["lightweight_functionality"] = {
                "status": "PASS" if success_count >= 2 and result.returncode == 0 else "FAIL",
                "execution_time_seconds": round(execution_time, 2),
                "return_code": result.returncode,
                "success_indicators_found": success_count,
                "output_size_chars": len(output)
            }
            
            self.performance_metrics["demo_execution_time"] = execution_time
            
            print(f"  ‚úÖ Execution time: {execution_time:.2f}s")
            print(f"  ‚úÖ Return code: {result.returncode}")
            print(f"  ‚úÖ Success indicators: {success_count}/3")
            
        except Exception as e:
            self.test_results["lightweight_functionality"] = {"status": "FAIL", "error": str(e)}
            print(f"  ‚ùå Lightweight functionality test failed: {e}")
    
    def _test_performance_benchmarks(self):
        """Test performance benchmarks and resource usage."""
        print("\n‚ö° Testing Performance Benchmarks...")
        
        try:
            benchmarks = {}
            
            # File I/O benchmark
            start_time = time.time()
            test_files = list(self.project_root.glob("**/*.py"))
            total_size = sum(f.stat().st_size for f in test_files)
            file_scan_time = time.time() - start_time
            benchmarks["file_scan"] = {"time_seconds": file_scan_time, "files_scanned": len(test_files)}
            
            # JSON processing benchmark
            start_time = time.time()
            test_data = {"circuits": [{"id": f"circuit_{i}", "params": {"value": i}} for i in range(1000)]}
            json_str = json.dumps(test_data)
            parsed_data = json.loads(json_str)
            json_processing_time = time.time() - start_time
            benchmarks["json_processing"] = {"time_seconds": json_processing_time, "objects_processed": 1000}
            
            # Memory usage estimation (simple heuristic)
            import sys
            memory_usage_mb = sys.getsizeof(test_data) / (1024 * 1024)
            benchmarks["memory_usage"] = {"test_data_mb": memory_usage_mb}
            
            # Performance thresholds
            performance_ok = (
                file_scan_time < 5.0 and
                json_processing_time < 1.0 and
                memory_usage_mb < 10.0
            )
            
            self.test_results["performance_benchmarks"] = {
                "status": "PASS" if performance_ok else "FAIL",
                "benchmarks": benchmarks
            }
            
            self.performance_metrics.update(benchmarks)
            
            print(f"  ‚úÖ File scan: {file_scan_time:.3f}s ({len(test_files)} files)")
            print(f"  ‚úÖ JSON processing: {json_processing_time:.3f}s (1000 objects)")
            print(f"  ‚úÖ Memory usage: {memory_usage_mb:.2f} MB")
            
        except Exception as e:
            self.test_results["performance_benchmarks"] = {"status": "FAIL", "error": str(e)}
            print(f"  ‚ùå Performance benchmark test failed: {e}")
    
    def _test_security_validation(self):
        """Test security validation and best practices."""
        print("\nüîí Testing Security Validation...")
        
        try:
            security_checks = {}
            
            # Check for hardcoded secrets (simple patterns)
            security_issues = []
            python_files = list(self.project_root.glob("**/*.py"))
            
            dangerous_patterns = [
                "password", "secret", "api_key", "token", "private_key"
            ]
            
            for py_file in python_files:
                try:
                    with open(py_file) as f:
                        content = f.read().lower()
                        for pattern in dangerous_patterns:
                            if f"{pattern} =" in content or f'"{pattern}"' in content:
                                security_issues.append(f"{py_file.name}: potential {pattern}")
                except:
                    continue
            
            # Check file permissions (basic check)
            executable_files = [f for f in python_files if os.access(f, os.X_OK)]
            
            security_checks = {
                "hardcoded_secrets": len(security_issues),
                "executable_python_files": len(executable_files),
                "total_files_scanned": len(python_files)
            }
            
            security_ok = len(security_issues) == 0
            
            self.test_results["security_validation"] = {
                "status": "PASS" if security_ok else "WARN",
                "checks": security_checks,
                "issues": security_issues[:5]  # Limit output
            }
            
            print(f"  ‚úÖ Files scanned: {len(python_files)}")
            print(f"  ‚úÖ Security issues: {len(security_issues)}")
            print(f"  ‚úÖ Executable files: {len(executable_files)}")
            
        except Exception as e:
            self.test_results["security_validation"] = {"status": "FAIL", "error": str(e)}
            print(f"  ‚ùå Security validation test failed: {e}")
    
    def _test_documentation_completeness(self):
        """Test documentation completeness and quality."""
        print("\nüìö Testing Documentation Completeness...")
        
        try:
            doc_files = list(self.project_root.glob("**/*.md"))
            required_docs = ["README.md", "CONTRIBUTING.md", "LICENSE"]
            
            found_docs = [doc.name for doc in doc_files]
            missing_docs = [doc for doc in required_docs if doc not in found_docs]
            
            # Check README quality
            readme_path = self.project_root / "README.md"
            readme_quality = {}
            
            if readme_path.exists():
                with open(readme_path) as f:
                    readme_content = f.read()
                    readme_quality = {
                        "length_chars": len(readme_content),
                        "has_installation": "installation" in readme_content.lower(),
                        "has_usage": "usage" in readme_content.lower() or "example" in readme_content.lower(),
                        "has_features": "feature" in readme_content.lower()
                    }
            
            doc_completeness = len(missing_docs) == 0 and readme_quality.get("length_chars", 0) > 1000
            
            self.test_results["documentation_completeness"] = {
                "status": "PASS" if doc_completeness else "WARN",
                "doc_files_found": len(doc_files),
                "missing_required_docs": missing_docs,
                "readme_quality": readme_quality
            }
            
            print(f"  ‚úÖ Documentation files: {len(doc_files)}")
            print(f"  ‚úÖ Missing required docs: {len(missing_docs)}")
            print(f"  ‚úÖ README quality score: {sum(readme_quality.values())}/4")
            
        except Exception as e:
            self.test_results["documentation_completeness"] = {"status": "FAIL", "error": str(e)}
            print(f"  ‚ùå Documentation test failed: {e}")
    
    def _execute_quality_gates(self):
        """Execute comprehensive quality gates."""
        print("\nüö™ Executing Quality Gates...")
        
        # Gate 1: Core functionality
        core_functionality = self.test_results.get("lightweight_functionality", {}).get("status") == "PASS"
        
        # Gate 2: Code quality
        code_quality = self.test_results.get("code_quality", {}).get("status") == "PASS"
        
        # Gate 3: Performance
        performance_ok = self.performance_metrics.get("demo_execution_time", 999) < 30.0
        
        # Gate 4: Security
        security_ok = self.test_results.get("security_validation", {}).get("status") in ["PASS", "WARN"]
        
        # Gate 5: Test coverage
        test_coverage = sum(1 for test in self.test_results.values() if test.get("status") == "PASS")
        coverage_ok = test_coverage >= 4  # At least 4 out of 6 tests should pass
        
        self.quality_gates = {
            "core_functionality": core_functionality,
            "code_quality": code_quality,
            "performance": performance_ok,
            "security": security_ok,
            "test_coverage": coverage_ok
        }
        
        gates_passed = sum(self.quality_gates.values())
        gates_total = len(self.quality_gates)
        
        for gate_name, passed in self.quality_gates.items():
            status = "‚úÖ PASS" if passed else "‚ùå FAIL"
            print(f"  {gate_name.replace('_', ' ').title()}: {status}")
        
        print(f"\nüéØ Quality Gates: {gates_passed}/{gates_total} passed ({gates_passed/gates_total*100:.1f}%)")
        
        return gates_passed >= 4  # Require at least 4/5 gates to pass
    
    def _generate_test_report(self) -> Dict[str, Any]:
        """Generate comprehensive test report."""
        end_time = datetime.now()
        execution_time = (end_time - self.start_time).total_seconds()
        
        # Calculate overall status
        gates_passed = sum(self.quality_gates.values())
        overall_status = "PASS" if gates_passed >= 4 else "FAIL"
        
        report = {
            "genrf_cicd_test_report": {
                "timestamp": end_time.isoformat(),
                "execution_time_seconds": round(execution_time, 2),
                "overall_status": overall_status,
                "quality_gates": {
                    "passed": gates_passed,
                    "total": len(self.quality_gates),
                    "percentage": round(gates_passed / len(self.quality_gates) * 100, 1),
                    "details": self.quality_gates
                },
                "test_results": self.test_results,
                "performance_metrics": self.performance_metrics,
                "environment": {
                    "python_version": sys.version,
                    "platform": sys.platform,
                    "working_directory": str(self.project_root)
                }
            }
        }
        
        # Save report
        report_file = self.project_root / "genrf_cicd_test_report.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"\nüìä CI/CD Test Report Generated")
        print("=" * 60)
        print(f"Overall Status: {'üü¢ PASS' if overall_status == 'PASS' else 'üî¥ FAIL'}")
        print(f"Execution Time: {execution_time:.2f} seconds")
        print(f"Quality Gates: {gates_passed}/{len(self.quality_gates)} passed")
        print(f"Report saved: {report_file}")
        
        return report

def main():
    """Main entry point for CI/CD test suite."""
    runner = CICDTestRunner()
    report = runner.run_all_tests()
    
    # Return appropriate exit code for CI/CD
    overall_status = report["genrf_cicd_test_report"]["overall_status"]
    exit_code = 0 if overall_status == "PASS" else 1
    
    print(f"\nüèÅ CI/CD Test Suite Completed (Exit Code: {exit_code})")
    return exit_code

if __name__ == "__main__":
    sys.exit(main())